{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset was constructed from two datasets of the US National Institute of Standards and Technology (NIST). The training set consists of handwritten digits from 250 different people, 50 percent high school students, and 50 percent employees from the Census Bureau. \n",
    "\n",
    "- Contains a training set of 60,000 examples and a test set of 10,000 examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/IsfrRWvbUdRny/giphy.gif\" width=\"480\" height=\"329\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the Data\n",
    "- The MNIST dataset is free to download from [here](http://yann.lecun.com/exdb/mnist/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively the code segment below can be used to download the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-images-idx3-ubyte.gz  already exists\n",
      "train-labels-idx1-ubyte.gz  already exists\n",
      "t10k-images-idx3-ubyte.gz  already exists\n",
      "t10k-labels-idx1-ubyte.gz  already exists\n",
      "All files are available\n"
     ]
    }
   ],
   "source": [
    "# Adapted from :\n",
    "# (1): https://www.youtube.com/watch?v=6xar6bxD80g\n",
    "\n",
    "import os,urllib.request\n",
    "\n",
    "# PROVIDE YOUR DOWNLOAD DIRECTORY HERE\n",
    "datapath = './data/MNISTData/'  \n",
    "\n",
    "# CREATING DOWNLOAD DIRECTORY\n",
    "if not os.path.exists(datapath):\n",
    "    os.makedirs(datapath)\n",
    "\n",
    "# URLS TO DOWNLOAD FROM\n",
    "urls = ['http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "       'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "       'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "       'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz']\n",
    "\n",
    "for url in urls:\n",
    "    filename = url.split('/')[-1]   # GET FILENAME\n",
    "    \n",
    "    if os.path.exists(datapath+filename):\n",
    "        print(filename, ' already exists')  # CHECK IF FILE EXISTS\n",
    "    else:\n",
    "        print('Downloading ',filename)\n",
    "        urllib.request.urlretrieve (url, datapath+filename) # DOWNLOAD FILE\n",
    "     \n",
    "print('All files are available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Format\n",
    "\n",
    "The MNIST dataset is provided in IDX format. IDX format is a binary file format that must be converted before we can use it our code.\n",
    "\n",
    "##### Why IDX?\n",
    "- Performance & Memory Optimization.\n",
    "\n",
    "Binary file formats perform siginificantly better than other file formats such as CSV. Considering the large volumn of data we're dealing with, IDX is ideal for ensuring compact memory utilization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  table {margin-left: 0 !important;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As given in the original [MNIST Database doccumentation](http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The basic format for images\n",
    "\n",
    "|Offset | Type               | Value           |   Description                   |\n",
    "|-------|--------------------|-----------------|---------------------------------|\n",
    "|0000   |4 byte integer      |0x00000801(2051) |magic number (MSB first)         |\n",
    "|0004   |4 byte integer      |10000 or 60000   |number of images (test or train) |\n",
    "|0008   |4 byte integer      |28               |number of rows                   |\n",
    "|0012   |4 byte integer      |28               |number of columns                |\n",
    "|0016   |unsigned byte       |??               |pixel intensity (0-255)          |\n",
    "|0017   |unsigned byte       |??               |pixel intensity (0-255)          |\n",
    "|...    |...                 |...              |...                              |\n",
    "|xxxx   |unsigned byte       |??               |pixel intensity (0-255)          |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As detailed above, the first four bytes in the image data is the (Magic Number)[https://en.wikipedia.org/wiki/Magic_number_(programming)]. This is a unique number that lets the file reader know what file format/protocol to expect. In our case the magic number, in decimal format, is 2051.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can read in the data and confirm this by doing the following:\n",
    "\n",
    "First we unzip the data. [Gzip](https://docs.python.org/3/library/gzip.html\n",
    ") is a python library that helps with reading compressed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://docs.python.org/3/library/gzip.html\n",
    "\n",
    "# Reading in compressed files\n",
    "import gzip\n",
    "# read as sequence of bytes\n",
    "# mimincs open command in python - but deals with compression\n",
    "with gzip.open('data/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    file_content = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(file_content) # check data type of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first byte (four elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x00\\x00\\x08\\x03'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints in hex --> \\x\n",
    "file_content[0:4] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a handle on our data, and can begin to make sense of its contents. One important thing to note is that the data is stored in [Big Endian format](https://en.wikipedia.org/wiki/Endianness). Essentially, this means the data is stored big-end first. When looking at multiple bytes, the first byte (lowest address) is the biggest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2051"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MNIST creators suggest reading using a 32 bit integer in highendian order(detailed in docss)\n",
    "# Big Endian format --> least significant bit first \n",
    "int.from_bytes(file_content[0:4], byteorder='big')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Magic number confirmed, wohoo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same method as above, we can continue to view the rest of the content detailed in the table by accessing the bytes at the relevant poistions.\n",
    "\n",
    "##### Checking the number of images\n",
    "The MNIST provided table states there should be 1000 images, this is doccumented in the second byte as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "int.from_bytes(file_content[4:8], byteorder='big')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirming the number of rows in the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int.from_bytes(file_content[8:12], byteorder='big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Confirming the number of columns in the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int.from_bytes(file_content[12:16], byteorder='big')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an undersanding of the dimensions and format of our data, we can begin parsing the images. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing the Data\n",
    "\n",
    "Each byte from 16th byte onward, contains pixel data, and the type of the data is unsigned byte, i.e. each pixel data is contained in a 8-bit binary digit, having value from 0 to 255.\n",
    "\n",
    "If we parse the 16th binary data byte, and convert to decimal, we will get a value from 0 to 255, which gives us the value of the pixel at the (0,0) co-ordinate of the first Image in the training data-set.\n",
    "\n",
    "Our image dimensions, as we discovered above - tell us that there will be 28 x 28 pixels in each image. So each image will be 786 bytes in size. We can use Numpy and Matplot to give us a visualisation of these pixels an how they come to form an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "image = ~np.array(list(file_content[16:800])).reshape(28,28).astype(np.uint8)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Labels\n",
    "\n",
    "  \n",
    "|Offset | Type               | Value           |   Description                   |\n",
    "|-------|--------------------|-----------------|---------------------------------|\n",
    "|0000   |4 byte integer      |0x00000801(2049) |magic number (MSB first)         |\n",
    "|0004   |4 byte integer      |10000 or 60000   |number of items (test or train)  |\n",
    "|0008   |unsigned byte       |??               |label                            |\n",
    "|0009   |unsigned byte       |??               |label                            |\n",
    "|...    |...                 |...              |...                              |\n",
    "|xxxx   |unsigned byte       |??               |label                            |\n",
    "\n",
    "\n",
    "The very same process can be used to read in & parse the corresponding image label data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "# unzip and read the bytes sequence\n",
    "with gzip.open('data/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check magic number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2049"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(labels[0:4], byteorder=\"big\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check that our first label correponds to the image we read in earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "int.from_bytes(labels[8:9], byteorder=\"big\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the whole thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created the following script that uses the same processign technique but applied to the entire dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define an enum, not essential but handy to avoid mispelling and niggly mistakes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.python.org/3/library/enum.html\n",
    "# not essential but useful\n",
    "# enum for constants relating to MNIST data\n",
    "from enum import Enum\n",
    "class MNIST(Enum):\n",
    "    IMAGES = 2051\n",
    "    LABELS = 2049\n",
    "    TRAIN = 'train'\n",
    "    TEST = 'test'\n",
    "    TEST_SIZE = 10000\n",
    "    TRAIN_SIZE = 60000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning processing on:  MNISTData\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: './data/MNISTData'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fcca405b4e07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Beginning processing on: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Unzip the .gz files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatapath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mfile_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\gzip.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mbinary_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"write\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: './data/MNISTData'"
     ]
    }
   ],
   "source": [
    "# Adapted from :\n",
    "# (1): https://www.youtube.com/watch?v=6xar6bxD80g\n",
    "# (2): https://github.com/ianmcloughlin/jupyter-teaching-notebooks/blob/master/mnist.ipynb\n",
    "# (3): https://medium.com/@mannasiladittya/converting-mnist-data-in-idx-format-to-python-numpy-array-5cb9126f99f1\n",
    "\n",
    "# Reading in compressed files\n",
    "import gzip\n",
    "import numpy as np\n",
    "# mimincs open command in python - but deals with compression\n",
    "import os,codecs\n",
    "\n",
    "\n",
    "# provide your directory with the extracted files here\n",
    "datapath = './data/'\n",
    "\n",
    "files = os.listdir(datapath)\n",
    "\n",
    "\n",
    "#convert byte to int\n",
    "def byte_to_int(b):\n",
    "    return int.from_bytes(b, byteorder='big')\n",
    "\n",
    "# handy structure to hold the data we want to parse\n",
    "mnist_data = {}\n",
    "# get every file in the data directory\n",
    "for file in files:\n",
    "    print('Beginning processing on: ', file)\n",
    "    # Unzip the .gz files\n",
    "    with gzip.open(datapath + file, 'rb') as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    # MAGIC NUMBER : determines whether the it's labels or images we're reading\n",
    "    type = byte_to_int(file_content[:4])\n",
    "    print(type)\n",
    "     # 4-7: LENGTH OF THE ARRAY (DIMENSION 0)\n",
    "    length = byte_to_int(file_content[4:8])\n",
    "\n",
    "    # if dealing with the images\n",
    "    if (type == MNIST.IMAGES.value):\n",
    "        category = MNIST.IMAGES.name\n",
    "        num_rows = byte_to_int(file_content[8:12])  # Num of rows  (DIMENSION 1)\n",
    "        num_cols = byte_to_int(file_content[12:16])  # num of cols  (DIMENSION 2)\n",
    "\n",
    "        # read the pixel values as integers, offset account for meta data\n",
    "        parsed = np.frombuffer(file_content,dtype = np.uint8, offset = 16)\n",
    "        # Reshape the array [NO_OF_SAMPLES x HEIGHT x WIDTH]\n",
    "        parsed = parsed.reshape(length,num_rows,num_cols)\n",
    "    # otherwise dealing with label set\n",
    "    elif(type == MNIST.LABELS.value):\n",
    "        category = MNIST.LABELS.name\n",
    "        parsed = np.frombuffer(file_content, dtype=np.uint8, offset=8)\n",
    "        # reshape the array as number of samples\n",
    "        parsed = parsed.reshape(length)\n",
    "    # separate test data from training data - we know which is which from mnist doc detailing set size\n",
    "    if (length == MNIST.TEST_SIZE.value):\n",
    "        set = 'test'\n",
    "    elif (length==MNIST.TRAIN_SIZE.value):\n",
    "        set = 'train'\n",
    "    mnist_data[set+'_'+category] = parsed  # Save the parsed data into a dict for convenience\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then use the python Pillow library to save the images as pngs. This is done by converting the np array containing the pixel values.I chose to save each image in a folder named with the corresponding label number. After running the script, it should be sctructured as follows: \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-8-d52aba3ebb6a>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-d52aba3ebb6a>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    mnist_images\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    " ...\n",
    "    mnist_images\n",
    "    ├── test/\n",
    "        ├── 0\n",
    "        ├── 1\n",
    "        ├── ...\n",
    "        ├── 9\n",
    "    ├── train/\n",
    "        ├── 0\n",
    "        ├── 1\n",
    "        ├── ...\n",
    "        ├── 9\n",
    " ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train_IMAGES'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0e950cc83e9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msets\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m# for train and test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMAGES\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLABELS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# num of samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'train_IMAGES'"
     ]
    }
   ],
   "source": [
    "# PIL to save np array as image\n",
    "from PIL import Image\n",
    "\n",
    "# path where image will be saved to\n",
    "image_dir = './mnist_images/'\n",
    "\n",
    "sets = ['train','test']\n",
    "\n",
    "for set in sets:   # for train and test set\n",
    "    images = mnist_data[set+'_' + MNIST.IMAGES.name]\n",
    "    labels = mnist_data[set+'_'+ MNIST.LABELS.name]\n",
    "    # num of samples\n",
    "    no_of_samples = images.shape[0]\n",
    "     # for every sample\n",
    "    for indx in range (no_of_samples): \n",
    "        print(set, indx)\n",
    "        # get image\n",
    "        image = Image.fromarray(images[indx])\n",
    "        # get label\n",
    "        label = labels[indx]\n",
    "          # if directories do not exist then\n",
    "        if not os.path.exists(image_dir+set+'/'+str(label)+'/'):\n",
    "             # create train/test directory and class specific subdirectory\n",
    "            os.makedirs (image_dir+set+'/'+str(label)+'/')\n",
    "        filenumber = len(os.listdir(image_dir+set+'/'+str(label)+'/'))  # number of files in the directory for naming the file\n",
    "        # save the image with proper name\n",
    "        image.save(image_dir+set+'/'+str(label)+'/%05d.png'%(filenumber))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "    - https://betterexplained.com/articles/understanding-big-and-little-endian-byte-order/\n",
    "    - https://en.wikipedia.org/wiki/Endianness\n",
    "    - https://www.youtube.com/watch?v=6xar6bxD80g\n",
    "    - https://medium.com/@mannasiladittya/converting-mnist-data-in-idx-format-to-python-numpy-array-5cb9126f99f1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
